Mon 12 Feb 2018 07:20:31 PM PST

RESULT

k = "qmdp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:20
Simulating...100%|██████████████████████████████████████| Time: 0:01:35
reward: -10.545 ±  0.195
k = "ar_despot"
Creating Simulations...100%|████████████████████████████| Time: 0:07:46
Simulating...100%|██████████████████████████████████████| Time: 0:14:20
reward: -8.745 ±  0.180
k = "pomcpow"
Creating Simulations...100%|████████████████████████████| Time: 0:00:19
Simulating...100%|██████████████████████████████████████| Time: 0:11:33
reward: -9.951 ±  0.176
k = "pomcpdpw"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:14:14
reward: -10.746 ±  0.189
k = "pft"
Creating Simulations...100%|████████████████████████████| Time: 0:00:12
Simulating...100%|██████████████████████████████████████| Time: 0:11:46
reward: -11.910 ±  0.161
k = "pomcp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:26:17
reward: -14.276 ±  0.211
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Monday_12_Feb_18_59.csv...

Mon 12 Feb 2018 10:39:56 AM PST

RESULT

zsunberg@tula:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 38 icaps_2018/vdptag2_table.jl 
max_time = 1.0 = 1.0
max_depth = 10 = 10
N = 1000 = 1000
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:03:30
reward: 14.228 ±  1.621
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:08:33
reward: 38.650 ±  0.848
k = "to_next"
Simulating...100%|██████████████████████████████████████| Time: 0:06:35
reward: -15.811 ±  0.438
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:06:53
reward: 34.617 ±  0.870
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:41:00
reward: -11.674 ±  0.516
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:46:20
reward: -13.107 ±  0.549
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:54:34
reward: -18.505 ±  0.265


Sat 10 Feb 2018 01:05:37 PM PST

pft opt for vdptag2

mean(combined[:mean_reward]) = 34.811950775101714
iteration 39
mean(d) = [8.64351, 17.79, 12.6579, 15.6675, 6.73998]
det(cov(d)) = 1.759567322143827e-8
ev = eigvals(cov(d)) = [0.000120243, 0.0171848, 0.0604909, 0.121802, 1.15574]
(eigvecs(cov(d)))[:, j] = [-0.38064, -0.583394, 0.611271, -0.343526, -0.151995]
(eigvecs(cov(d)))[:, j] = [0.878822, -0.441938, 0.0182528, -0.175703, -0.0340483]
(eigvecs(cov(d)))[:, j] = [0.0417059, -0.228413, 0.347697, 0.70925, 0.567591]
(eigvecs(cov(d)))[:, j] = [-0.24289, -0.435707, -0.589299, -0.289784, 0.565611]
(eigvecs(cov(d)))[:, j] = [0.148496, 0.471518, 0.397302, -0.513914, 0.577636]

Tue 30 Jan 2018 12:55:25 PM PST

pow_opt for vdptag2

mean(combined[:mean_reward]) = 39.34430949569718
iteration 21
mean(d) = [63.4931, 20.5716, 10.2502, 3.47125, 81.3509]
det(cov(d)) = 1.3200977801784242
ev = eigvals(cov(d)) = [0.016337, 0.0239187, 1.29561, 14.828, 175.848]
(eigvecs(cov(d)))[:, j] = [0.0070858, 0.184705, -0.959766, 0.191787, -0.0888853]
(eigvecs(cov(d)))[:, j] = [0.0813041, 0.0143906, 0.197992, 0.976702, 0.00592561]
(eigvecs(cov(d)))[:, j] = [-0.0113952, -0.982572, -0.177943, 0.0515584, -0.0100712]
(eigvecs(cov(d)))[:, j] = [0.780962, -0.0147681, 0.0461558, -0.0703953, -0.618703]
(eigvecs(cov(d)))[:, j] = [0.619118, -0.00345987, -0.0765128, -0.0407115, 0.780493]

Sat 27 Jan 2018 04:45:16 PM PST

RESULT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/subhunt_table
.jl
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:49
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
k = "ar_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:03:21
reward: 27.092 ±  1.329
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:04:33
reward: 61.809 ±  1.367
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:05:16
reward: 27.801 ±  1.341
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:03:38
reward: 72.675 ±  1.205
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:03:19
reward: 27.185 ±  1.331
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 27.786 ±  1.340
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:04:36
reward: 27.980 ±  1.343
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_Saturday_27_Jan_13_26.csv...
ERROR: LoadError: UndefVarError: CSV not defined


Fri 26 Jan 2018 04:45:55 PM PST

RESULT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/subhunt_discr
etization.jl
max_depth = 20 = 20
max_time = 1.0 = 1.0
N = 1000 = 1000
max_time = 1.0
(k, binsize) = ("qmdp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:47
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:03:18
reward: 29.104 ±  1.356
(k, binsize) = ("d_pomcp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 28.353 ±  1.349
(k, binsize) = ("qmdp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:43
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:16
reward: 26.591 ±  1.324
(k, binsize) = ("d_pomcp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 28.251 ±  1.347
(k, binsize) = ("qmdp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:03:16
reward: 26.780 ±  1.327
(k, binsize) = ("d_pomcp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:04:34
reward: 28.425 ±  1.349
(k, binsize) = ("qmdp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:03:17
reward: 25.857 ±  1.313
(k, binsize) = ("d_pomcp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 28.340 ±  1.348
(k, binsize) = ("qmdp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:16
reward: 27.276 ±  1.332
(k, binsize) = ("d_pomcp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 27.080 ±  1.332
(k, binsize) = ("qmdp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:03:20
reward: 26.225 ±  1.318
(k, binsize) = ("d_pomcp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:33
reward: 28.087 ±  1.345
(k, binsize) = ("qmdp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:42
reward: 26.458 ±  1.323
(k, binsize) = ("d_pomcp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:30
reward: 28.750 ±  1.358
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_time_Friday_26_Jan_14_39.csv...
done.


Thu 25 Jan 2018 01:40:53 PM PST

mean(combined[:mean_reward]) = 61.4267447713561
iteration 16
mean(d) = [17.0653, 5.7884, 47.5693]
det(cov(d)) = 1.0654999602917252
ev = eigvals(cov(d)) = [0.0127303, 0.665979, 125.677]
(eigvecs(cov(d)))[:, j] = [-0.116846, 0.991147, -0.063049]
(eigvecs(cov(d)))[:, j] = [0.985356, 0.123633, 0.117421]
(eigvecs(cov(d)))[:, j] = [-0.124177, 0.0484054, 0.991079]

Wed 24 Jan 2018 05:30:45 PM PST

Subhunt pow opt stopped

mean(combined[:mean_reward]) = 53.971569571948294
iteration 5
mean(d) = [23.4068, 6.07487, 36.9538]
det(cov(d)) = 29773.25249346697
ev = eigvals(cov(d)) = [1.92731, 65.9505, 234.237]
(eigvecs(cov(d)))[:, j] = [-0.0361602, -0.989378, 0.1408]
(eigvecs(cov(d)))[:, j] = [0.991381, -0.0532668, -0.119691]
(eigvecs(cov(d)))[:, j] = [0.125919, 0.135258, 0.982776]
creating 60 simulation sets............................................................

Wed 24 Jan 2018 10:46:06 AM PST

Subhunt pow opt

mean(combined[:mean_reward]) = 47.534730786740525
iteration 14
mean(d) = [93.601, 7.5144, 16.4266]
det(cov(d)) = 0.488007248072637
ev = eigvals(cov(d)) = [0.0248916, 0.239055, 82.0117]
(eigvecs(cov(d)))[:, j] = [-0.0478393, 0.908884, 0.414295]
(eigvecs(cov(d)))[:, j] = [-0.341551, -0.404653, 0.848292]
(eigvecs(cov(d)))[:, j] = [0.938645, -0.100921, 0.329789]

Tue 23 Jan 2018 06:04:10 PM PST

with N = 1000

zsunberg@tula:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 38 icaps_2018/simpleld_table.jl 
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
k = "heuristic_01"
Simulating...100%|██████████████████████████████████████| Time: 0:00:08
reward: 24.469 ±  0.854
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:05:19
reward:  6.659 ±  1.267
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  5.287 ±  1.248
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:04:04
reward: 62.179 ±  0.511
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:34:07
reward:  5.297 ±  1.254
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 57.123 ±  0.396
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:03:42
reward: 64.496 ±  0.383
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:09:21
reward: 52.163 ±  1.346
k = "heuristic_1"
Simulating...100%|██████████████████████████████████████| Time: 0:00:03
reward: 28.216 ±  0.898
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:32:52
reward:  4.491 ±  1.238


Tue 23 Jan 2018 03:52:38 PM PST

RESULT: LD Discretization

zsunberg@tula:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 38 icaps_2018/ld_discretizatio
n.jl
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
(k, d) = ("qmdp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:16
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:08
reward: 62.052 ±  0.496
(k, d) = ("d_pomcp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:24
reward: 61.234 ±  0.561
(k, d) = ("d_despot", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:09:12
reward: -22.312 ±  2.513
(k, d) = ("qmdp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:04:09
reward: 62.153 ±  0.438
(k, d) = ("d_pomcp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:41
reward: 64.365 ±  0.419
(k, d) = ("d_despot", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:09:05
reward: -4.192 ±  2.451
(k, d) = ("qmdp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:04:13
reward: 62.234 ±  0.433
(k, d) = ("d_pomcp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:04:60
reward: 62.244 ±  0.573
(k, d) = ("d_despot", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:09:40
reward:  7.768 ±  2.297
(k, d) = ("qmdp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:04:08
reward: 62.127 ±  0.504
(k, d) = ("d_pomcp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:26:48
reward: 13.883 ±  1.289            
(k, d) = ("d_despot", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:09:29
reward: 23.042 ±  2.112
(k, d) = ("qmdp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:38
reward: 58.972 ±  0.605
(k, d) = ("d_pomcp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:11:47
reward: 26.626 ±  0.801
(k, d) = ("d_despot", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:09:20
reward: 51.608 ±  1.374
(k, d) = ("qmdp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:13
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:51
reward: 57.205 ±  0.717
(k, d) = ("d_pomcp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:21:46
reward: 23.825 ±  1.350
(k, d) = ("d_despot", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:09:28
reward: 51.933 ±  1.383
(k, d) = ("qmdp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:06:19
reward: 51.525 ±  0.912
(k, d) = ("d_pomcp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:51
reward: 60.444 ±  0.498
(k, d) = ("d_despot", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:09:28
reward: 42.864 ±  1.711



Tue 23 Jan 2018 10:45:59 AM PST

mean(combined[:mean_reward]) = 61.980279863198554
iteration 47
mean(d) = [91.4278, 4.73208, 14.3109]
det(cov(d)) = 5.48831984150069e-8
ev = eigvals(cov(d)) = [4.31236e-7, 0.0197767, 6.43531]
(eigvecs(cov(d)))[:, j] = [-0.0269423, 0.929454, -0.367953]
(eigvecs(cov(d)))[:, j] = [0.0695339, 0.368938, 0.926849]
(eigvecs(cov(d)))[:, j] = [0.997216, -0.000613751, -0.0745686]

Mon 22 Jan 2018 02:12:00 PM PST

pow_opt for simple light-dark

mean(combined[:mean_reward]) = 61.649340998087304
iteration 10
mean(d) = [115.716, 4.18574, 13.7728]
ev = eigvals(cov(d)) = [0.202858, 1.1569, 621.191]
(eigvecs(cov(d)))[:, j] = [0.00148127, 0.999641, -0.0267648]
(eigvecs(cov(d)))[:, j] = [0.046333, 0.0266675, 0.99857]
(eigvecs(cov(d)))[:, j] = [0.998925, -0.00271925, -0.0462768]

Thu 18 Jan 2018 03:21:14 PM PST

RESULT: GOOD LASERTAG

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/lasertag_table.jl 
max_time = 1.0 = 1.0
max_depth = 90 = 90
exploration = 26.0 = 26.0
N = 1000 = 1000
k = "ar_despot"
Creating Simulations...100%|████████████████████████████| Time: 0:07:29
Simulating...100%|██████████████████████████████████████| Time: 0:13:47
reward: -8.919 ±  0.188
k = "pomcpow"
Creating Simulations...100%|████████████████████████████| Time: 0:00:28
Simulating...100%|██████████████████████████████████████| Time: 0:12:20
reward: -10.283 ±  0.182
k = "pomcpdpw"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:14:20
reward: -10.428 ±  0.202
k = "pft"
Creating Simulations...100%|████████████████████████████| Time: 0:00:12
Simulating... 26%|██████████                            |  ETA: 0:08:44^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[Simulating...100%|██████████████████████████████████████| Time: 0:11:05
reward: -11.586 ±  0.166
k = "pomcp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:27:05
reward: -14.497 ±  0.213
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Thursday_18_Jan_15_16.csv...
done.
zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/lasertag_table.jl 
max_time = 1.0 = 1.0
max_depth = 90 = 90
exploration = 26.0 = 26.0
N = 1000 = 1000
k = "qmdp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:20
Simulating...100%|██████████████████████████████████████| Time: 0:01:37
reward: -10.486 ±  0.204
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Thursday_18_Jan_15_20.csv...
done.

Thu 11 Jan 2018 10:16:27 AM PST

Discretization preview

zach@Theresa:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018$ julia -p 6 ld_discretization.jl 
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 100 = 100
(k, d) = ("qmdp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:18
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:51
reward: 43.628 ±  2.737
(k, d) = ("d_despot", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:03:12
reward: 26.876 ±  6.423
(k, d) = ("qmdp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:13
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:06:03
reward: 40.452 ±  2.953
(k, d) = ("d_despot", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:09
reward: 23.634 ±  6.640
(k, d) = ("qmdp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:06:00
reward: 41.328 ±  2.530
(k, d) = ("d_despot", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:03:09
reward: 27.366 ±  6.400
(k, d) = ("qmdp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:13
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:05:27
reward: 43.276 ±  2.464
(k, d) = ("d_despot", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:03:08
reward: 23.861 ±  6.655
(k, d) = ("qmdp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 43.687 ±  2.447
(k, d) = ("d_despot", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:07
reward: 32.498 ±  6.166
(k, d) = ("qmdp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:43
reward: 45.848 ±  2.185
(k, d) = ("d_despot", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:03:07
reward: 28.075 ±  6.377
(k, d) = ("qmdp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:51
reward: 43.919 ±  2.123
(k, d) = ("d_despot", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:08
reward: 27.151 ±  6.431


Thu 11 Jan 2018 10:13:09 AM PST

Laser Tag Table:

with exploration = 25
(oops: pomcpow used FORollout instead of FOValue

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 27 icaps_2018/lasertag_table.jl 
max_time = 1.0 = 1.0
max_depth = 90 = 90
N = 1000 = 1000
k = "ar_despot"
Creating Simulations...100%|████████████████████████████| Time: 0:10:01
Simulating...100%|██████████████████████████████████████| Time: 0:21:34
reward: -8.977 ±  0.188
k = "pomcpow"
Creating Simulations...100%|████████████████████████████| Time: 0:00:42
Simulating...100%|██████████████████████████████████████| Time: 0:39:19
reward: -13.219 ±  0.226
k = "pomcpdpw"
Creating Simulations...100%|████████████████████████████| Time: 0:00:23
Simulating...100%|██████████████████████████████████████| Time: 0:25:44
reward: -10.751 ±  0.193
k = "pft"
Creating Simulations...100%|████████████████████████████| Time: 0:00:23
Simulating...100%|██████████████████████████████████████| Time: 0:20:40
reward: -11.736 ±  0.155
k = "pomcp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:21
Simulating...100%|██████████████████████████████████████| Time: 0:50:60
reward: -15.441 ±  0.195
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Thursday_11_Jan_02_35.csv...
done.


Mon 04 Dec 2017 05:11:41 PM PST

mean(combined[:mean_reward]) = -9.947048511007113
iteration 88
mean(d) = [26.3468, 3.62511, 35.0585]
ev = eigvals(cov(d)) = [2.93607e-11, 6.44598e-8, 0.00381188]
(eigvecs(cov(d)))[:, j] = [0.00247583, -0.998464, -0.0553574]
(eigvecs(cov(d)))[:, j] = [0.381521, -0.0502272, 0.922995]
(eigvecs(cov(d)))[:, j] = [-0.924357, -0.0234052, 0.38081]

Fri 01 Dec 2017 01:50:51 PM PST

Woo - discretized DESPOT works with -100, 100 bounds, and it doesn't work TOO well!

zach@Theresa:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 4 simpleld/compare.jl 
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 100 = 100
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:05:26
reward: 33.394 ±  6.001

Thu 30 Nov 2017 07:41:52 PM PST

Lasertag parameter optimization (with different problems at each iter)

iteration 8
mean(d) = [83.1987, 4.23302, 14.2264]
eigvals(cov(d)) = [0.0490264, 4.54873, 1051.37]
eigvecs(cov(d)) = [-0.0110958 0.00449547 0.999928; 0.99624 0.0859784 0.0106683; -0.0859243 0.996287 -0.00543256]
creating 80 simulation sets................................................................................
Simulating...100%|██████████████████████████████████████| Time: 0:43:31
mean(combined[:mean_reward]) = -12.143111447669922

Thu 30 Nov 2017 10:17:26 AM PST

Lasertag parameter optimization
(running on the same 80 problems each time)

Simulating...100%|██████████████████████████████████████| Time: 0:46:57
mean(combined[:mean_reward]) = -12.885963286130709
iteration 24
mean(d) = [69.2461, -2.111, 10.5063]
eigvals(cov(d)) = [0.0479565, 0.436726, 4.66563]
eigvecs(cov(d)) = [-0.101515 -0.465128 -0.879404; -0.160202 -0.864791 0.475892; 0.98185 -0.189192 -0.0132753]

Wed 22 Nov 2017 03:26:06 AM PST

simple LD results
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 500 = 500
k = "heuristic_01"
Simulating...100%|██████████████████████████████████████| Time: 0:00:04
reward: 28.599 ±  1.268
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:02:07
reward:  2.363 ±  1.737
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:05
reward:  2.499 ±  1.709
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:01:37
reward: 62.532 ±  0.715
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:14:06
reward:  2.892 ±  1.709
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:01:43
reward: 58.188 ±  0.523
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:04:52
reward: 30.054 ±  1.145
k = "d_despot"

Tue 21 Nov 2017 10:42:36 PM PST

Results for VDPTag 2

Tue 21 Nov 2017 05:44:55 PM PST

Acceptable results for the table for subhunt

max_time = 2.0 = 2.0
max_depth = 20 = 20
N = 1000 = 1000
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:46
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
k = "ar_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:05:39
reward: 26.745 ±  1.325
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:08:12
reward: 45.500 ±  1.458
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:09:17
reward: 27.810 ±  1.342
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:06:45
reward: 74.784 ±  1.163
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:05:39
reward: 27.099 ±  1.329
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:08:45
reward: 28.159 ±  1.346
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:08:45
reward: 28.166 ±  1.346
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_Tuesday_21_Nov_17_39.csv...
done.


Tue 21 Nov 2017 04:44:42 PM PST

Ok, I think we're good with the new VDPTag
max_time = 1.0 = 1.0
max_depth = 10 = 10
N = 500 = 500
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:01:08
reward: 11.808 ±  2.287
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:03:39
reward: 40.189 ±  1.119
k = "to_next"
Simulating...100%|██████████████████████████████████████| Time: 0:01:52
reward: -16.876 ±  0.547
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:03:19
reward: 31.420 ±  1.316

Mon 20 Nov 2017 04:07:36 PM PST

Wooooo!

max_time = 2.0 = 2.0
max_depth = 20 = 20
N = 500 = 500
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:26
reward: 27.339 ±  1.888
k = "despot_0.1_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:12
reward: 26.780 ±  1.877
k = "despot_3.0_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:12
reward: 26.637 ±  1.876
k = "despot_2.0_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:10
reward: 27.116 ±  1.882
k = "despot_10.0_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:40
reward: 27.359 ±  1.890
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:04:17
reward: 54.165 ±  2.031
k = "despot_0.5_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:09
reward: 26.065 ±  1.863
k = "despot_5.0_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:12
reward: 24.404 ±  1.825
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:12
reward: 26.025 ±  1.860
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:16
reward: 80.111 ±  1.486
k = "despot_1.0_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:10
reward: 27.135 ±  1.883
k = "despot_20.0_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:51
reward: 27.808 ±  1.902

Sun 19 Nov 2017 08:13:42 AM PST

Sweet! with K=1000, k=4, and alpha=1/10

max_time = 10.0 = 10.0
max_depth = 20 = 20
N = 500 = 500
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:10:41
reward: 25.369 ±  1.850
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:22
reward: 27.339 ±  1.888
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:16
reward: 80.111 ±  1.486
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:20:13
reward: 43.446 ±  2.058

Sun 19 Nov 2017 12:49:18 AM PST

with p_aware_kill = 0.6, somewhat better

max_time = 10.0 = 10.0
max_depth = 20 = 20
N = 500 = 500
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:10:41
reward: 25.369 ±  1.850
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:22
reward: 27.339 ±  1.888
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:16
reward: 80.111 ±  1.486
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:20:13
reward: 43.446 ±  2.058

Sat 18 Nov 2017 11:23:36 PM PST

First subhunt... shoot

N = 500 = 500
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:01:49
reward: 25.513 ±  1.851
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:22
reward: 27.339 ±  1.888
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:17
reward: 72.263 ±  1.751
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:02:37
reward: 27.568 ±  1.895


Fri 17 Nov 2017 09:16:24 PM PST

N = 1000
solvers["despot"].lambda = 0.01
solvers["despot"].K = 500
solvers["despot"].T_max = 1.0
solvers["pomcpow"].max_time = 1.0
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:53
mean(rewards) = -10.807104307124584
std(rewards) / sqrt(N) = 0.1947013045198957
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:12:35
mean(rewards) = -10.920345876333318
std(rewards) / sqrt(N) = 0.16474969966372327
k = "despot"
Simulating...  1%|                                      |  ETA: 0:34:00WARNING: DESPOT's MemorizingSource
 random number generator had to move the memory locations of the rngs 457 times. If this number was large
, it may be affecting performance (profiling is the best way to tell).

To suppress this warning, use MemorizingSource(..., move_warning=false).

To reduce the number of moves, try using MemorizingSource(..., min_reserve=n) and increase n until the nu
mber of moves is low.


Fri 17 Nov 2017 07:48:28 PM PST

[ ] try pomcpow with better parameters
[ ] 

Tue 14 Nov 2017 10:35:54 PM PST

Simple light-dark

max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 20 = 20
k = "heuristic_01"
Simulating...100%|██████████████████████████████████████| Time: 0:00:06
reward: -12.400 ±  4.695
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward: -77.450 ±  8.921
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:03
reward: -74.250 ±  9.713
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:00:43
reward:  3.650 ±  0.577
k = "despot_0"
Simulating...100%|██████████████████████████████████████| Time: 0:00:07
reward: -77.450 ±  8.921
k = "heuristic_1"
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: -12.400 ±  4.695

Fri 10 Nov 2017 11:04:34 PM PST

with random numbers set

iteration 76
mean(d) = [62.9585, 18.1685, 13.0588, -0.837702, 41.5676]
eigvals(cov(d)) = [9.6914e-5, 0.00060545, 0.0842944, 0.551136, 0.711184]
eigvecs(cov(d)) = [-0.0786876 0.503232 -0.471822 -0.703022 0.153983; 0.960925 -0.139087 -0.197108 -0.0469298 0.127373; -0.19985 -0.846533 -0.169606 -0.434554 0.160746; -0.114573 -0.0917033 -0.822258 0.421748 -0.352822; -0.131775 0.0488395 -0.183461 0.369932 0.899857]
Simulating...100%|██████████████████████████████████████| Time: 0:03:40
mean(combined[:mean_reward]) = 41.016346066769394

Tue 07 Nov 2017 06:01:52 AM PST

cross-entropy maximization

iteration 100
mean(d) = [66.4341, 12.783, 7.83781, -2.42884, 33.3333]
eigvals(cov(d)) = [6.79305e-7, 0.00137267, 0.00604985, 0.241371, 2.35293]
eigvecs(cov(d)) = [0.152955 0.142402 0.639559 0.438939 -0.595503; -0.906431 -0.198838 0.202791 -0.218463 -0.223599; 0.304718 0.0804533 0.0342595 -0.822607 -0.472036; -0.245514 0.951867 -0.181687 0.00852072 -0.0242896; 0.0430942 0.166321 0.718093 -0.287839 0.609895]


Fri 20 Oct 2017 02:30:45 PM PDT

This one is with K=100

pomcpow (10, 10)
Simulating...100%|██████████████████████████████████████| Time: 0:00:58
reward: 19.824 ±  0.875
despot (10, 10)
Simulating...100%|██████████████████████████████████████| Time: 0:02:02
reward:  5.328 ±  0.936
pomcp (10, 10)
Simulating...100%|██████████████████████████████████████| Time: 0:02:05
reward:  0.833 ±  0.840
pomcpow (32, 32)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 34.254 ±  0.902
despot (32, 32)
Simulating...100%|██████████████████████████████████████| Time: 0:00:40
reward: 42.110 ±  0.705
pomcp (32, 32)
Simulating...100%|██████████████████████████████████████| Time: 0:02:17
reward: -6.254 ±  0.672
pomcpow (100, 100)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 35.776 ±  0.888
despot (100, 100)
Simulating...100%|██████████████████████████████████████| Time: 0:02:11
reward: 22.899 ±  0.794
pomcp (100, 100)
Simulating...100%|██████████████████████████████████████| Time: 0:01:45

Fri 20 Oct 2017 02:30:13 PM PDT

[Below is with K=500 so it exceeds the 0.1 time limit]

Fri 20 Oct 2017 11:03:27 AM PDT

Oh, shoot...

pomcpow (100, 100)
Simulating...100%|██████████████████████████████████████| Time: 0:00:40
reward: 38.094 ±  0.826
despot (100, 100)
Simulating...100%|██████████████████████████████████████| Time: 0:03:04
reward: 51.876 ±  0.455
pomcp (100, 100)
Simulating...100%|██████████████████████████████████████| Time: 0:01:44
reward:  6.676 ±  0.774

Tue 17 Oct 2017 04:54:10 PM PDT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 lasertag/discrete.jl 
N = 1000
solvers["despot"].lambda = 0.01
solvers["despot"].K = 500
solvers["despot"].T_max = 2.0
solvers["pomcpow"].max_time = 2.0
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:59
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:23:44
mean(rewards) = -10.554632699622246
std(rewards) / sqrt(N) = 0.16924298684972508
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:22:46
mean(rewards) = -8.670399408477682
std(rewards) / sqrt(N) = 0.18508926759230465
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Tuesday_17_Oct_15_59.jld...
done.


Thu 05 Oct 2017 10:02:18 AM PDT

Whoah - this works really well!!
Does the random policy also include random tags??

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 500 -t 1.0 -p 0.01 --ubtype MDP --lbtype RANDOM
Will eventually write to data/cpp_run_Thu_05_Oct_09_16.txt...
using args -n 500 -t 1.0 -p 0.01 --ubtype MDP --lbtype RANDOM
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/0.4s  
Average total discounted reward (stderr) = -8.493163 (0.130435)


Wed 04 Oct 2017 03:45:22 PM PDT

Ok, so to make something compelling, we have to implement the good bounds exactly, which means understanding exactly how this works.
Then we need to make a harder continuous problem
also speed tests

Wed 04 Oct 2017 02:05:38 PM PDT

TEST: does lambda decrease performance with MDP/TRIVIAL
ANSWER: lolwut. yes.

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 500 -t 1.0 -p 0.01 --ubtype MDP --lbtype TRIVIAL
Will eventually write to data/cpp_run_Wed_04_Oct_14_43.txt...
using args -n 500 -t 1.0 -p 0.01 --ubtype MDP --lbtype TRIVIAL
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:56/1142/100%/1.0s ^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^local:0/2000/100%/1.0s  ^[[3~^[[3~[B^[[B^[[B^[[B[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B
Average total discounted reward (stderr) = -14.074241 (0.191560)

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 500 -t 1.0 -p 0.0 --ubtype MDP --lbtype TRIVIAL
Will eventually write to data/cpp_run_Wed_04_Oct_14_01.txt...
using args -n 500 -t 1.0 -p 0.0 --ubtype MDP --lbtype TRIVIAL
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/0.1s  
Average total discounted reward (stderr) = -10.781712 (0.141706)


zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 500 -t 1.0 -p 0.0 --ubtype MDP --lbtype RANDOM
Will eventually write to data/cpp_run_Wed_04_Oct_14_08.txt...
using args -n 500 -t 1.0 -p 0.0 --ubtype MDP --lbtype RANDOM
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/0.6s  [[3~
Average total discounted reward (stderr) = -10.475910 (0.136797)

Tue 03 Oct 2017

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 500 -t 10
Will eventually write to data/cpp_run_Tue_03_Oct_15_50.txt...
using args -n 500 -t 10
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/1.7s  
Average total discounted reward (stderr) = -8.720817 (0.128947)

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 5000 -t 10
Will eventually write to data/cpp_run_Tue_03_Oct_13_39.txt...
using args -n 5000 -t 10
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/2.1s  
Average total discounted reward (stderr) = -8.842181 (0.136431)

Tue 03 Oct 2017 01:39:29 PM PDT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 100 -t 0.2
Will eventually write to data/cpp_run_Tue_03_Oct_13_33.txt...
using args -n 100 -t 0.2
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/0.1s  
Average total discounted reward (stderr) = -9.398212 (0.126560)

Tue 03 Oct 2017 01:32:51 PM PDT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 500 -t 1
Will eventually write to data/cpp_run_Tue_03_Oct_13_07.txt...
using args -n 500 -t 1
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/0.4s  
Average total discounted reward (stderr) = -8.566343 (0.131037)


Tue 03 Oct 2017 12:03:51 PM PDT

with -n 5000 -t 10
cpp_run_Mon_02_Oct_20_20
rewards: -9.017 +/- 0.133

with -n 500 -t 1

Mon 02 Oct 2017 01:14:06 PM PDT

Trying to increase the max time with K

K = 5000
N = 1000
T_max = 10.0
k = "ardespot"
Simulating...100%|██████████████████████████████████████| Time: 0:23:36
mean(rewards) = -11.730302144935402
std(rewards) / sqrt(N) = 0.20165559476130868
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:42
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:49:44
mean(rewards) = -11.699963803538598
std(rewards) / sqrt(N) = 0.20202869275535995
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/despot_comparison_run_Monday_2_Oct_14_35.jld...

K = 500
N = 1000
T_max = 1.0
k = "ardespot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:54
mean(rewards) = -11.02352406445708
std(rewards) / sqrt(N) = 0.19018357382631232
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:43
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:02:05
mean(rewards) = -11.16220935441867
std(rewards) / sqrt(N) = 0.19411016062998934
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/despot_comparison_run_Mond
ay_2_Oct_13_11.jld...
done.


Mon 02 Oct 2017 01:01:55 PM PDT

Ok, need to run c++ despot with k=5000 and time = 10 sec?
Should probably parallelize

Mon 02 Oct 2017 11:37:59 AM PDT

lolwut

K = 5000
N = 1000
k = "ardespot"
Simulating...100%|██████████████████████████████████████| Time: 0:20:05
mean(rewards) = -11.730302144935402
std(rewards) / sqrt(N) = 0.20165559476130868
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:42
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:44:27
mean(rewards) = -11.773622775128977
std(rewards) / sqrt(N) = 0.20137185094246154
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/despot_comparison_run_Monday_2_Oct_12_53.jld...
done.


Despot comparison at K=500

N = 1000
k = "ardespot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:54
mean(rewards) = -11.02352406445708
std(rewards) / sqrt(N) = 0.19018357382631232
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:43
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:02:04
mean(rewards) = -11.09346871146992
std(rewards) / sqrt(N) = 0.19182739340444913
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/despot_comparison_run_Monday_2_Oct_11_35.jld...
done.

Fri 29 Sep 2017 09:36:08 AM PDT

At 10 it is worse

N = 1000
solvers["despot"].lambda = 0.0
solvers["despot"].K = 10
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:50
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:09
mean(rewards) = -11.419435630794528
std(rewards) / sqrt(N) = 0.17382558562220637
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Thursday_28_Sep_21_36.jld...

but at 50 it seems to be really good

N = 1000
solvers["despot"].lambda = 0.0
solvers["despot"].K = 50
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:50
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:08
mean(rewards) = -10.668149214716083
std(rewards) / sqrt(N) = 0.1797646933082757
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Thursday_28_Sep_22_04.jld...
done.

Thu 28 Sep 2017 09:31:33 PM PDT

??? Decreasing K leads to better performance

N = 1000
solvers["despot"].lambda = 0.0
solvers["despot"].K = 100
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:50
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:13
mean(rewards) = -10.85445958803187
std(rewards) / sqrt(N) = 0.18417685723205504
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Thursday_28_Sep_21_31.jld...
done.


Thu 28 Sep 2017 06:26:57 PM PDT

Something is very, very strange with K

solvers["despot"].lambda = 0.0
solvers["despot"].K = 50000
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:21
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:25:06
mean(rewards) = -18.32498586378628
std(rewards) / sqrt(N) = 0.5187454940785285
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Thursday_28_Sep_19_57.jld...

solvers["despot"].lambda = 0.0
solvers["despot"].K = 5000
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:22
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:39
mean(rewards) = -11.639852993326196
std(rewards) / sqrt(N) = 0.672522296221273
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Thursday_28_Sep_18_23.jld...
done.

solvers["despot"].lambda = 0.0
solvers["despot"].K = 500
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:21
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:00:16
mean(rewards) = -10.889650128672468
std(rewards) / sqrt(N) = 0.6570518347344256


Thu 28 Sep 2017 09:52:03 AM PDT

With old bounds

lambda = 0.0
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:20
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:07:58
mean(rewards) = -10.479284095896972
std(rewards) / sqrt(N) = 0.6509758212436103
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Thursday_28_Sep_09_50.jld...
done.

Thu 28 Sep 2017 09:28:40 AM PDT

With new bounds

WHY SO FAST??

lambda = 0.01
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:20
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:00:17
mean(rewards) = -13.028315239540785
std(rewards) / sqrt(N) = 0.6681122080230456
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Wednesday_27_Sep_23_19.jld...
done.

lambda = 0.0
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:20
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:00:16
mean(rewards) = -10.889650128672468
std(rewards) / sqrt(N) = 0.6570518347344256
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Wednesday_27_Sep_23_25.jld...


Mon 25 Sep 2017 02:18:39 PM PDT

N = 100
lambda = 0.01
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:20
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:08:15
mean(rewards) = -12.847838247957897
std(rewards) / sqrt(N) = 0.8827825430347918
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Wednesday_20_Sep_19_32.jld...
done.

Fri 01 Sep 2017 02:46:33 PM PDT

VDP Tag
[X] Discretization
    [X] Plot discretization fine-ness
[X] Time trend
    [X] POMCPOW time limit
    [X] POMCP time limit

Tue 29 Aug 2017 06:56:57 PM PDT

N = 100
n = 1000000
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:25
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "pomcpow"
Simulating... 97%|█████████████████████████████████████ |  ETA: 0:01:26gvim: Fatal IO error 11 (Resource temporarily unavailable) on X server localhost:10.0.
Simulating...100%|██████████████████████████████████████| Time: 0:53:16
mean(rewards) = -9.5652359501832
std(rewards) / sqrt(N) = 0.5432442305531421
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:54
mean(rewards) = -23.419155908663896
std(rewards) / sqrt(N) = 2.3841840480092236
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:53:43
mean(rewards) = -10.73522490448752
std(rewards) / sqrt(N) = 0.7414608817904751
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Monday_28_Aug_23_07.jld...
done.

Thu 10 Aug 2017 02:21:02 PM PDT

N=100
n=100000
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:10:35
mean(rewards) = -12.070827601297673
std(rewards) / sqrt(N) = 0.7035574068690779


Wed 02 Aug 2017 09:19:29 PM PDT

N=100?
n=500000?
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 1:13:06
mean(rewards) = -10.606161592470805
std(rewards) / sqrt(N) = 0.6399314663360224


Wed 02 Aug 2017 05:07:51 PM PDT

n = 100_000
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:31:58
1918.487838 seconds (114.44 k allocations: 8.852 MiB)
mean(rewards) = -10.753207384302518
std(rewards) / sqrt(N) = 0.6451073093825108

Mon 05 Jun 2017 11:13:12 AM PDT

cpp despot with seed 4
Q...#......
.#.........
...........
...........
...#.......
..##...#.#.
...#.......

Sat 03 Jun 2017 05:17:16 PM PDT

[X] Look and see if any obvious parameters are off
    ? eta
[ ] Better heuristic?
[ ] Manually Verify?
[ ] Reproduce in C++
[ ] Make despot faster

Sat 03 Jun 2017 01:07:41 PM PDT

Why isn't DESPOT working well enough?

N = 100
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:32
mean(rewards) = -12.690541696637322
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 1:48:21
mean(rewards) = -13.04114149677848
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/laser_pomcpow_run_Saturday_3_Jun_02_31.jld...


Simulating...100%|██████████████████████████████████████| Time: 0:00:30
k = "qmdp"
mean(rewards) = -12.422425647531348
Simulating...100%|██████████████████████████████████████| Time: 1:15:60
k = "pomcpow"
mean(rewards) = -11.779366647775527
Simulating...100%|██████████████████████████████████████| Time: 1:49:08
k = "despot"
mean(rewards) = -12.954366015757728
Simulating...100%|██████████████████████████████████████| Time: 0:00:21
k = "move_towards_sampled"
mean(rewards) = -14.148419970637537
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/laser_pomcpow_run_Sunday_21_May_22_58.jld...


Sat 20 May 2017 10:58:43 PM PDT

With 500_000 iters
Simulating...100%|██████████████████████████████████████| Time: 0:00:32
k = "qmdp"
mean(rewards) = -12.422425647531348
Simulating...100%|██████████████████████████████████████| Time: 1:18:19
k = "pomcpow"
mean(rewards) = -11.719120527120975
Simulating...100%|██████████████████████████████████████| Time: 0:00:20
k = "move_towards_sampled"
mean(rewards) = -14.148419970637537

Sat 20 May 2017 03:13:06 PM PDT

less aggressive dpw
enable_action_pw=true,
k_action=4.0,
alpha_action=1/8,
Simulating...100%|██████████████████████████████████████| Time: 0:05:24
k = "qmdp"
mean(rewards) = -12.789484244325477
Simulating...100%|██████████████████████████████████████| Time: 1:24:11
k = "pomcpow"
mean(rewards) = -13.675970735083467
Simulating...100%|██████████████████████████████████████| Time: 0:03:03
k = "move_towards_sampled"
mean(rewards) = -15.060258754472738
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/laser_pomcpow_run_Saturday_20_May_16_56.jld...



Sat 20 May 2017 03:11:27 PM PDT

Fixed dynamics; without action PW

Simulating...100%|██████████████████████████████████████| Time: 0:05:22
k = "qmdp"
mean(rewards) = -12.789484244325477
Simulating...100%|██████████████████████████████████████| Time: 1:38:42
k = "pomcpow"
mean(rewards) = -12.447929052529078
Simulating...100%|██████████████████████████████████████| Time: 0:03:01
k = "move_towards_sampled"
mean(rewards) = -15.060258754472738

Sat 20 May 2017 01:12:04 PM PDT

After fixing dynamics, and with action PW,

enable_action_pw=true,
k_action=4.0,
alpha_action=1/20,

Simulating...100%|██████████████████████████████████████| Time: 0:05:22
k = "qmdp"
mean(rewards) = -12.789484244325477
Simulating...100%|██████████████████████████████████████| Time: 1:28:58
k = "pomcpow"
mean(rewards) = -14.357106698498535
Simulating...100%|██████████████████████████████████████| Time: 0:03:04
k = "move_towards_sampled"
mean(rewards) = -15.060258754472738

Fri 19 May 2017 10:59:14 AM PDT

Packages that will need updating
POMCP
POMCPOW
LaserTag

Fri 19 May 2017 09:20:47 AM PDT

LaserTag:

Simulating...100%|██████████████████████████████████████| Time: 1:39:13
k = "pomcpow"
mean(rewards) = -13.319609000942341
Simulating...100%|██████████████████████████████████████| Time: 0:03:15
k = "move_towards_sampled"
mean(rewards) = -15.087908579615986

Thu 18 May 2017 09:49:53 AM PDT

[ ] Random obstacles
[ ] Visualization
[ ] Discretized
[ ] POMCP
[ ] DESPOT

Fri 12 May 2017 07:05:57 PM PDT

Forgot to copy and paste the screen, but it seems like the discretization does marginally worse
at n = 10000
discretized POMCP -> 15
pomcpow -> 20

Thu 11 May 2017 11:22:45 PM PDT

MDP discrete-doesn't seems to matter too much

zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 20 vdptag/mcts_discretization.jl
N = 1000 = 1000
Simulating...100%|██████████████████████████████████████| Time: 0:00:06
k = "discrete_random"
mean(rewards) = -16.99111470314662
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
k = "discrete_heur"
mean(rewards) = 26.058640290930313
Simulating...100%|██████████████████████████████████████| Time: 0:10:12
k = "continuous_dpw"
mean(rewards) = 50.321368047118916
Simulating...100%|██████████████████████████████████████| Time: 0:16:49
k = "discrete_dpw"
mean(rewards) = 52.87807537064958
Simulating...100%|██████████████████████████████████████| Time: 0:20:45
k = "discrete_mcts"
mean(rewards) = 56.599055225113
Simulating...100%|██████████████████████████████████████| Time: 0:00:00
k = "continuous_heur"
mean(rewards) = 46.710457390363665

Wed 10 May 2017 11:26:24 AM PDT

MDP trends vs discrete
POMDP trends
vs Discrete
rewrite POMCP???
rewrite visualization???

Tue 09 May 2017 02:27:40 PM PDT

zsunberg@cambridge:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 10 vdptag/mcts_computation_trend.jl
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
ns[j] = 10 ^ (j / 2) = 3.1622776601683795
mrew[j] = mean(rewards) = -16.034891224228613
Simulating...100%|██████████████████████████████████████| Time: 0:00:04
ns[j] = 10 ^ (j / 2) = 10.0
mrew[j] = mean(rewards) = -10.729625355402273
Simulating...100%|██████████████████████████████████████| Time: 0:00:07
ns[j] = 10 ^ (j / 2) = 31.622776601683793
mrew[j] = mean(rewards) = 2.7637845588229917
Simulating...100%|██████████████████████████████████████| Time: 0:00:14
ns[j] = 10 ^ (j / 2) = 100.0
mrew[j] = mean(rewards) = 13.585877878818392
Simulating...100%|██████████████████████████████████████| Time: 0:00:26
ns[j] = 10 ^ (j / 2) = 316.2277660168379
mrew[j] = mean(rewards) = 30.993963847238497
Simulating...100%|██████████████████████████████████████| Time: 0:00:57
ns[j] = 10 ^ (j / 2) = 1000.0
mrew[j] = mean(rewards) = 41.132460035880214
Simulating...100%|██████████████████████████████████████| Time: 0:02:32
ns[j] = 10 ^ (j / 2) = 3162.2776601683795
mrew[j] = mean(rewards) = 45.61063910859104
Simulating...100%|██████████████████████████████████████| Time: 0:07:04
ns[j] = 10 ^ (j / 2) = 10000.0
mrew[j] = mean(rewards) = 50.353312334532276
Simulating...100%|██████████████████████████████████████| Time: 0:22:10
ns[j] = 10 ^ (j / 2) = 31622.776601683792
mrew[j] = mean(rewards) = 51.66214703274198
Simulating...100%|██████████████████████████████████████| Time: 1:04:45
ns[j] = 10 ^ (j / 2) = 100000.0
mrew[j] = mean(rewards) = 54.93820326422724
       ┌────────────────────────────────────────────────────────────┐   
    60 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ y1
       │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⡠⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠒⠒⠒⠒⠒⠒⠒⠒⠒⠒⠒⠒│   
       │⠀⠀⠀⠀⡠⠔⠒⠒⠒⠒⠒⠒⠊⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⠀⡠⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⢰⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡜⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡏⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠁│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
   -20 │⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       └────────────────────────────────────────────────────────────┘   
       0                                                       100000

Sun 07 May 2017 07:15:40 PM PDT

What tests to run????
First thoroughly check out VDP Tag and see what else we might want to investigate

DESPOT on lightdark
POMCP on lightdark?

Discretized POMCP, DESPOT, POMCP-DPW

Benchmarks from other papers


Thu 30 Mar 2017 10:34:13 PM PDT

after changing to 0.5 timestep
n=1000: 32.42
n=100:

Thu 30 Mar 2017 08:30:17 PM PDT

vdp mcts with n=1000
8.016
with n=100, 4.39

Wed 29 Mar 2017 07:03:52 PM PDT

XXX USE XXX

zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 20 uai_2017/ld_comparison.jl
pomcpow_10k (1 of 2)...100%|████████████████████████████| Time: 0:01:13
mean(rewards[sk]) = -9.267762047007196
sum(counts[sk]) / sum(steps[sk]) = 18464.847734227016
greedy (2 of 2)...100%|█████████████████████████████████| Time: 0:00:54
mean(rewards[sk]) = -15.753999479537239
sum(counts[sk]) / sum(steps[sk]) = 0.0
pomcpow_10k mean: -9.267762047007196 sem: 0.1707708909033004
pomcpow_10k time: 2.564552 sim counts: 253448.5 steps: 13.726
greedy mean: -15.753999479537239 sem: 0.14423709957881475
greedy time: 2.141464 sim counts: 0.0 steps: 33.686
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Wednesday_29_Mar_19_04.jld...
done.

Wed 29 Mar 2017 06:55:39 PM PDT

XXX USE XXX

zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 20 uai_2017/ld_comparison.jl
heuristic (1 of 6)...100%|██████████████████████████████| Time: 0:00:09
mean(rewards[sk]) = -7.033655189262772
sum(counts[sk]) / sum(steps[sk]) = 0.0
pomcpow (2 of 6)...100%|████████████████████████████████| Time: 1:19:26
mean(rewards[sk]) = -6.377730449077106
sum(counts[sk]) / sum(steps[sk]) = 2.4143038177150195e6
pomcpdpw (3 of 6)...100%|███████████████████████████████| Time: 7:33:26
mean(rewards[sk]) = -15.69943647695567
sum(counts[sk]) / sum(steps[sk]) = 319307.70218304446
bt_100_ro_100k (4 of 6)...100%|█████████████████████████| Time: 1:03:53
mean(rewards[sk]) = -5.613691760365924
sum(counts[sk]) / sum(steps[sk]) = 2.4490792149007633e7
bt_100_osv_100k (5 of 6)...100%|████████████████████████| Time: 0:47:07
mean(rewards[sk]) = -6.85277809582549
sum(counts[sk]) / sum(steps[sk]) = 8.728218322816644e6
greedy (6 of 6)...100%|█████████████████████████████████| Time: 0:00:56
mean(rewards[sk]) = -15.753999479537239
sum(counts[sk]) / sum(steps[sk]) = 0.0
heuristic mean: -7.033655189262772 sem: 0.10013644232719489
heuristic time: 0.12171999999999979 sim counts: 0.0 steps: 8.984
pomcpow mean: -6.377730449077106 sem: 0.09845233232867533
pomcpow time: 184.11048000000005 sim counts: 1.880742674e7 steps: 7.79
pomcpdpw mean: -15.69943647695567 sem: 0.1560158316692792
pomcpdpw time: 1066.607432 sim counts: 1.079451618e7 steps: 33.806
bt_100_ro_100k mean: -5.613691760365924 sem: 0.07078723454023751
bt_100_ro_100k time: 150.52855200000002 sim counts: 1.60414688576e8 steps: 6.55
bt_100_osv_100k mean: -6.85277809582549 sem: 0.12512502756233473
bt_100_osv_100k time: 109.29648800000001 sim counts: 7.6354453888e7 steps: 8.748
greedy mean: -15.753999479537239 sem: 0.14423709957881475
greedy time: 2.195144 sim counts: 0.0 steps: 33.686
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Wednesday_29_Mar_10_05.jld...
done.

Tue 28 Mar 2017 10:52:03 PM PDT

zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 20 uai_2017/ld_comparison.jl  bt_1000_osv_10k (1 of 9)...100%|████████████████████████| Time: 0:13:47
mean(rewards[sk]) = -11.212760358096592
sum(counts[sk]) / sum(steps[sk]) = 9.317113126448894e6
heuristic (2 of 9)...100%|██████████████████████████████| Time: 0:00:02
mean(rewards[sk]) = -7.226265473715379
sum(counts[sk]) / sum(steps[sk]) = 0.0
bt_100_ro_50k (3 of 9)...100%|██████████████████████████| Time: 0:08:40
mean(rewards[sk]) = -8.450746172323269
sum(counts[sk]) / sum(steps[sk]) = 4.5615460781512605e6
pomcpow (4 of 9)...100%|████████████████████████████████| Time: 0:17:33
mean(rewards[sk]) = -6.427691278976738
sum(counts[sk]) / sum(steps[sk]) = 2.4160163984771576e6
pomcpdpw (5 of 9)...100%|███████████████████████████████| Time: 1:40:42
mean(rewards[sk]) = -15.853267090626584
sum(counts[sk]) / sum(steps[sk]) = 319143.984297761
bt_10000_osv_1k (6 of 9)...100%|████████████████████████| Time: 0:10:28
mean(rewards[sk]) = -9.363843614136982
sum(counts[sk]) / sum(steps[sk]) = 9.489309755780347e6
bt_1000_ro_5k (7 of 9)...100%|██████████████████████████| Time: 0:06:02
mean(rewards[sk]) = -10.135463216813116
sum(counts[sk]) / sum(steps[sk]) = 4.621683939315688e6
bt_100_osv_100k (8 of 9)...100%|████████████████████████| Time: 0:11:30
mean(rewards[sk]) = -7.268103412923633
sum(counts[sk]) / sum(steps[sk]) = 8.624871826923076e6
greedy (9 of 9)...100%|█████████████████████████████████| Time: 0:00:13
mean(rewards[sk]) = -15.893171937096822
sum(counts[sk]) / sum(steps[sk]) = 0.0
bt_1000_osv_10k mean: -11.212760358096592 sem: 0.44127950612902006
bt_1000_osv_10k time: 134.07095999999999 sim counts: 1.7683880714e8 steps: 18.98
heuristic mean: -7.226265473715379 sem: 0.2725311133802234
heuristic time: 0.3929199999999998 sim counts: 0.0 steps: 9.59
bt_100_ro_50k mean: -8.450746172323269 sem: 0.35955894726607157
bt_100_ro_50k time: 70.87136 sim counts: 5.428239833e7 steps: 11.9
pomcpow mean: -6.427691278976738 sem: 0.23038042462137528
pomcpow time: 195.14068 sim counts: 1.903820922e7 steps: 7.88
pomcpdpw mean: -15.853267090626584 sem: 0.34130456588620695
pomcpdpw time: 1097.0013600000002 sim counts: 1.097536162e7 steps: 34.39
bt_10000_osv_1k mean: -9.363843614136982 sem: 0.3648557809087868
bt_10000_osv_1k time: 90.44756 sim counts: 1.3133204702e8 steps: 13.84
bt_1000_ro_5k mean: -10.135463216813116 sem: 0.3857735894005351
bt_1000_ro_5k time: 56.43364 sim counts: 7.158988422e7 steps: 15.49
bt_100_osv_100k mean: -7.268103412923633 sem: 0.28570066391523014
bt_100_osv_100k time: 114.95456 sim counts: 8.07288003e7 steps: 9.36
greedy mean: -15.893171937096822 sem: 0.30374049951337256
greedy time: 2.2675600000000005 sim counts: 0.0 steps: 34.11
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Monday_27_Mar_02_14.jld...
done.

Sun 26 Mar 2017 10:33:37 PM PDT

for bt_osv_2500, we should use

2500*100 + 2500 = 252500


Sun 26 Mar 2017 07:28:04 PM PDT

zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 20 uai_2017/ld_comparison.jl 
bt_osv_2500 (1 of 3)...100%|████████████████████████████| Time: 0:09:11
mean(rewards[sk]) = -9.708126278280934
sum(counts[sk]) / sum(steps[sk]) = 2375.5079695079694
heuristic (2 of 3)...100%|██████████████████████████████| Time: 0:00:02
mean(rewards[sk]) = -7.226265473715379
sum(counts[sk]) / sum(steps[sk]) = 0.0
greedy (3 of 3)...100%|█████████████████████████████████| Time: 0:00:13
mean(rewards[sk]) = -15.893171937096822
sum(counts[sk]) / sum(steps[sk]) = 0.0
bt_osv_2500 mean: -9.708126278280934 sem: 0.3601598549097225
bt_osv_2500 time: 83.33244 sim counts: 34278.58 steps: 14.43
heuristic mean: -7.226265473715379 sem: 0.2725311133802234
heuristic time: 0.4150799999999999 sim counts: 0.0 steps: 9.59
greedy mean: -15.893171937096822 sem: 0.30374049951337256
greedy time: 2.2194 sim counts: 0.0 steps: 34.11
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Sunday_26_Mar_19_25.jld...
done.


Sun 26 Mar 2017 04:54:04 PM PDT

modified_pomcp_2500 (1 of 5)...100%|████████████████████| Time: 0:04:22
mean(rewards[sk]) = -6.228689810256388
heuristic (2 of 5)...100%|██████████████████████████████| Time: 0:00:02
mean(rewards[sk]) = -7.235164761413105
pomcpow (3 of 5)...100%|████████████████████████████████| Time: 0:21:46
mean(rewards[sk]) = -7.165346048548632
modified_pomcp (4 of 5)...100%|█████████████████████████| Time: 0:15:53
mean(rewards[sk]) = -6.310329385942821
greedy (5 of 5)...100%|█████████████████████████████████| Time: 0:00:12
mean(rewards[sk]) = -15.905852704954135
modified_pomcp_2500 mean: -6.228689810256388 sem: 0.18214953179338803
modified_pomcp_2500 time: 43.771240000000006 sim counts: 1.4972938632e8 steps: 7.46
heuristic mean: -7.235164761413105 sem: 0.27554282336930086
heuristic time: 0.38867999999999975 sim counts: 0.0 steps: 9.64
pomcpow mean: -7.165346048548632 sem: 0.27300311644202
pomcpow time: 225.59455999999997 sim counts: 2.205615447e7 steps: 9.23
modified_pomcp mean: -6.310329385942821 sem: 0.22239423722155335
modified_pomcp time: 162.78972000000005 sim counts: 3.6961437985e8 steps: 7.76
greedy mean: -15.905852704954135 sem: 0.305511430505056
greedy time: 2.24324 sim counts: 0.0 steps: 34.18
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Sunday_26_Mar_16_47.jld...
done.

Sun 26 Mar 2017 03:17:15 PM PDT

modified pomcp still does better than pomcpow with the same amount of simulations

Fri 24 Mar 2017 10:53:01 AM PDT

zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 20 uai_2017/ld_comparison.jl
heuristic (1 of 5)...100%|██████████████████████████████| Time: 0:00:08
mean(rewards[sk]) = -7.235164761413105
pomcpow (2 of 5)...100%|████████████████████████████████| Time: 0:17:41
mean(rewards[sk]) = -6.464334592747194
modified_pomcp (3 of 5)...100%|█████████████████████████| Time: 0:14:19
mean(rewards[sk]) = -6.046531249173305
vanilla_pomcp (4 of 5)...100%|██████████████████████████| Time: 1:43:55
mean(rewards[sk]) = -15.698370015626965
greedy (5 of 5)...100%|█████████████████████████████████| Time: 0:00:13
mean(rewards[sk]) = -16.08387713284787
heuristic mean: -7.235164761413105 sem: 0.27554282336930086
heuristic time: 0.45967999999999987 sim counts: 0.0 steps: 9.64
pomcpow mean: -6.464334592747194 sem: 0.2422449509537489
pomcpow time: 191.7218 sim counts: 1.948591693e7 steps: 8.03
modified_pomcp mean: -6.046531249173305 sem: 0.17713801841733476
modified_pomcp time: 151.79852 sim counts: 3.4263578421e8 steps: 7.19
vanilla_pomcp mean: -15.698370015626965 sem: 0.32712445611450214
vanilla_pomcp time: 1065.6900799999999 sim counts: 1.079682272e7 steps: 33.43
greedy mean: -16.08387713284787 sem: 0.27354707984413584
greedy time: 2.3991200000000004 sim counts: 0.0 steps: 34.67
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Friday_24_Mar_00_52.jld...
done.


Wed 22 Mar 2017 09:32:00 PM PDT


heuristic (1 of 5)...100%|██████████████████████████████| Time: 0:00:07
pomcpow (2 of 5)...100%|████████████████████████████████| Time: 0:17:36
modified_pomcp (3 of 5)...100%|█████████████████████████| Time: 0:21:54
vanilla_pomcp (4 of 5)...100%|██████████████████████████| Time: 1:49:08
greedy (5 of 5)...100%|█████████████████████████████████| Time: 0:00:13
heuristic mean: -7.235164761413105 sem: 0.27554282336930086
heuristic time: 0.44763999999999987 sim counts: 0.0
pomcpow mean: -6.509998772902603 sem: 0.24299155377625786
pomcpow time: 190.81344 sim counts: 2.021049328e7
modified_pomcp mean: -6.313697990295132 sem: 0.2325999439490863
modified_pomcp time: 163.98836 sim counts: 3.7512339338e8
vanilla_pomcp mean: -15.472871746113288 sem: 0.3934097251852203
vanilla_pomcp time: 1060.45316 sim counts: 1.406729023e7
greedy mean: -16.00305793721465 sem: 0.2763833162506445
greedy time: 2.3548 sim counts: 3.40177267e6
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Wednesday_22_Mar_17_25.jld...
done.


Wed 15 Mar 2017 12:27:40 PM PDT

500_000 particles in pomcpow
heuristic (1 of 5)...100%|██████████████████████████████| Time: 0:00:07
pomcpow (2 of 5)...100%|████████████████████████████████| Time: 0:16:29
modified_pomcp (3 of 5)...100%|█████████████████████████| Time: 0:23:31
vanilla_pomcp (4 of 5)...100%|██████████████████████████| Time: 0:12:50
greedy (5 of 5)...100%|█████████████████████████████████| Time: 0:00:21
heuristic mean: -7.235164761413105 sem: 0.27554282336930086
pomcpow mean: -7.469768562232542 sem: 0.29152126787795163
modified_pomcp mean: -5.756805160244173 sem: 0.20696151441664185
vanilla_pomcp mean: -16.39413058834327 sem: 0.27982087036589837
greedy mean: -15.613544378353634 sem: 0.33457615002693186


1_000_000 particles in pomcpow
zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 10  --color=yes scratch/eval_pomcp.jl
heuristic (1 of 5)...100%|██████████████████████████████| Time: 0:00:08
pomcpow (2 of 5)...100%|████████████████████████████████| Time: 0:40:07
modified_pomcp (3 of 5)...100%|█████████████████████████| Time: 0:24:49
vanilla_pomcp (4 of 5)...100%|██████████████████████████| Time: 0:12:51
greedy (5 of 5)...100%|█████████████████████████████████| Time: 0:00:22
heuristic mean: -7.235164761413105 sem: 0.27554282336930086
pomcpow mean: -6.679350450167447 sem: 0.27008009511089115
modified_pomcp mean: -5.909027501851269 sem: 0.2175159081441871
vanilla_pomcp mean: -16.46974571625036 sem: 0.25654700388065066
greedy mean: -15.613544378353634 sem: 0.33457615002693186

switched to SimpleParticleFilter for modified
heuristic (1 of 4)...100%|██████████████████████████████| Time: 0:00:07
modified_pomcp (2 of 4)...100%|█████████████████████████| Time: 0:19:24
vanilla_pomcp (3 of 4)...100%|██████████████████████████| Time: 0:12:58
greedy (4 of 4)...100%|█████████████████████████████████| Time: 0:00:22
heuristic mean: -7.235164761413105 sem: 0.27554282336930086
modified_pomcp mean: -6.267678192270309 sem: 0.18293426883608033
vanilla_pomcp mean: -16.55849811620158 sem: 0.25824830249995767
greedy mean: -15.613544378353634 sem: 0.33457615002693186

Mon 06 Feb 2017 12:15:06 PM PST

[ ] Make parallel script for just one solver
[ ] Test it
[ ] Profile?
[ ] Wait for Particle Filter update

